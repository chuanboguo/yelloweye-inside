
```{r round2-function}
round2 <- function(x) sprintf("%.2f",round(x, 2))
```

*TODO: Take Step 1-5 results out of here and put in appropriate files*

## STEP 6: PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE  {#sec:approach6}

*TODO: all the text is Rex text (Rext). Change to Yelloweye.*

### STEP 1: DEFINE THE DECISION CONTEXT {#sec:results1}

*TODO: Do we need this step here? Should be in the methods*

The decision to be made is which MP to use to determine catch limits for the period until the next available catch advice. 
TODO write more here or there should be in the methods.

### STEP 2: SELECTION OF OBJECTIVES AND PERFORMANCE METRICS  {#sec:results2}

We defined the objectives for Yelloweye Rockfish based on
TODO: write more here.

TODO: list the objectives here

1. Maintain ...
2. Maintain ...
3. Etc.

TODO: justify length of projection period here (based on gen time)

TEXT about choice of 100 y projection period and generation time (check Quang's notes on years used for calculating LT performance metrics)


The performance metrics measuring the objectives were:

TODO: list the performance metrics here

<!--
Rex:
1. LT LRP: Probability *B* > 0.4 *B*~MSY~ (years 35--50)
2. LT USR: Probability *B* > 0.8 *B*~MSY~ (years 35--50)
3. FMSY: P(*F* < *F*~MSY~) (years 1--50)
4. STC: Probability catch > reference catch (years 1--10)
5. LTC: Probability catch > reference catch (years 35--50)
6. AADC: Probability AADC (average absolute interannual difference in catch) < historical AADC (years 1--50)
-->

We measured the performance metrics as probabilities integrated over years and replicates simultaneously.

### STEP 3: SELECTION OF UNCERTAINTIES/SPECIFICATION OF OPERATING MODELS {#sec:results3}

We considered the major axes of uncertainty for Yelloweye Rockfish to be:

TODO: list these here:

We divided OM scenarios into a reference set and a robustness set (Section \@ref(sec:best3) *REFER TO MP FRAMEWORK*). We defined six OM reference-set scenarios that differed from each other in these respects and two OM robustness-set scenarios (Table \@ref(tab:scen)), with parameter values provided in Appendix \@ref(app:desc-om-yelloweye). We describe each of the scenarios below.

```{r scen, results='asis'}
sc <- readRDS(here("mse/om/ye-scenarios.rds"))
sc %>%
  select(-order) %>%
  mutate(scenario_human = gsub("\\\n", " ", scenario_human)) %>%
  select(-scenario) %>%
  csasdown::csas_table(caption = "Yelloweye Rockfish OM scenarios. ",
    col_names = c("Scenario name", "Set type")) %>%
  kableExtra::kable_styling(latex_options = c("hold_position"))
```

### Reference Set {#sec:results-reference-set}

#### Upweight dogfish survey

Descriptive text, possibly figs

#### Low catch

Descriptive text, possibly figs

#### Episodic recruitment

Descriptive text, possibly figs

#### Estimate HBLL selectivity

Descriptive text, possibly figs

### The Robustness Set {#sec:results-robustness-set}

#### Low natural mortality ($M$)

We considered one alternative scenario that differed in terms of the value of $M$ (Table \@ref(tab:scen)).

TODO: write more here.

#### High projected CV in HBLL

TODO: write more here.

### OPERATING MODEL CONDITIONING

After specifying most of the OM parameters (Appendix \@ref(app:desc-om-yelloweye)), we conditioned the OMs using the SRA model described in Appendix B of @anderson2020gfmp.

TODO: write more here.

The SRA was able to fit to the HBLL survey relative biomass index and TODO indices reasonably well for all OM scenarios (Figure \@ref(fig:survey-fits)).
The TODO survey was better fit than the TODO, likely due to TODO (Figure \@ref(fig:survey-fits)).
The SRA models fit the catch data almost perfectly by design, via setting the standard deviations of the observation error to a value of 0.01 [@anderson2020gfmp, their Equation B.27].

(ref:fig-survey-fits)
SRA model fits to the HBLL, Dogfish, and three commercial CPUE relative  indices.
Panels from left to right represent OM scenarios.
Thin lines represent individual SRA model fits across stochastic draws from the various OM parameters.
Dots represent index mean and line segments represent 2 times the standard errors as entered into the SRA models.

```{r survey-fits, fig.cap="(ref:fig-survey-fits)", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-index-fits.png"))
```

We used the SRA to populate the following parameters in the conditioned OMs:

- $B_{t_c}/B_0$ (or "D"; depletion in the last historical year)
- $R_0$ (unfished recruitment)
- $\theta_\textrm{AC}$ (or "AC"; first-order autocorrelation of recruitment deviations)
- $F_{a,y}$ (fishing mortality at age by year)
- $\varepsilon_{\textrm{R},y}$ for years $t_1$ to $t_c$ (historical recruitment deviations)

The reference and robustness scenarios rendered a range of estimated parameter values (Figures \@ref(fig:sra-conditioned-parameters), \@ref(fig:F-om), \@ref(fig:recdev-om)).

TODO: note that all replicates converged

Other key parameters that were possibly affected by the SRA (but there wasn't any filtering) (Figure \@ref(fig:sra-conditioned-parameters2)).

The implied spawning stock biomass depletion trajectories during the historical period from the eight OMs were TODO (Figure \@ref(fig:depletion-om)).


```{r sra-fraction, results='asis', eval=FALSE}
# This just checks that all replicas converged
rex_converged <- readRDS(here("mse/om/ye-converged.rds"))
rex_converged %>%
  mutate(fraction_converged = sprintf("%.2f",round(nsim / 250, 2))) %>%
  mutate(scenario = gsub("\\n", " ", scenario)) %>%
  select(-nsim) %>%
  csasdown::csas_table(col_names = c("Scenario", "Fraction retained"),
    caption = "Fraction of replicates retained after conditioning with the SRA model.")
```

*TODO: make ye-sra-estimated.png and ye-sra-filtered.png*

(ref:fig-sra-conditioned-parameters) Histograms of parameters estimated by the SRA. Other parameters: Fishing mortality at age, the historical depletion trajectory, and historical recruitment deviations are also derived from the SRA model. Historical depletion trajectories are shown in Figure \@ref(fig:depletion-om), apical fishing mortality by year are shown in Figure \@ref(fig:F-om), and historical recruitment deviations are shown in Figure \@ref(fig:recdev-om).
D refers to depletion. AC refers to $\theta_\textrm{AC}$.

```{r sra-conditioned-parameters, out.width="0.85\\textwidth", fig.cap="(ref:fig-sra-conditioned-parameters)"}
knitr::include_graphics(here("mse/figures/ye-sra-estimated.png"))
```

(ref:fig-sra-conditioned-parameters2)
Frequency polygons for parameters input to the SRA. Parameters were sampled stochastically as input into the SRA (see Appendix \@ref(app:desc-om-rex) for ranges). Any samples associated with non-converged SRA models were discarded. Since most SRA models converged, these distributions are similar across scenarios unless they were specified differently in the OM. The large spikes represent fixed parameter values in the OM. sigma_R refers to $\sigma_R$.

```{r sra-conditioned-parameters2, out.width="\\textwidth", fig.asp=0.85, fig.width=7, fig.cap="(ref:fig-sra-conditioned-parameters2)"}
knitr::include_graphics(here("mse/figures/ye-sra-filtered.png"))
```

<!-- TODO: this fig is kind of weird with the triangles -->

\clearpage

(ref:fig-depletion-om)
Spawning stock biomass ($B$) depletion trajectories for reference and robustness set OMs.
Depletion is represented as a fraction of $B_0$ (spawning stock biomass at unfished equilibrium).
Lines represent medians, and dark and light grey shading represent 50% and 95% quantiles across replicates.

```{r depletion-om, fig.cap="(ref:fig-depletion-om)", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-compare-SRA-depletion-panel.png"))
```

(ref:fig-F-om)
Apical fishing mortality ($F$) trajectories for reference and robustness set OMs.
Apical fishing mortality is the maximum $F$ experienced by fish of any age in a given year.
Lines represent medians, and dark and light grey shading represent 50% and 95% quantiles across replicates.

```{r F-om, fig.cap="(ref:fig-F-om)", out.width="\\textwidth"}
knitr::include_graphics(here::here("mse/figures/ye-compare-SRA-F-panel.png"))
```

(ref:fig-recdev-om)
Historical recruitment deviations estimated by the SRA model (in log space).
Lines represent 100 random samples from the replicates.

```{r recdev-om, fig.cap="(ref:fig-recdev-om)", out.width="\\textwidth"}
knitr::include_graphics(here::here("mse/figures/ye-compare-SRA-recdev-panel.png"))
```

\clearpage

## STEP 4: IDENTIFICATION OF CANDIDATE MANAGEMENT PROCEDURES  {#sec:results4}

We started with the full set of provisional candidate MPs described in Section TODO (Table \@ref(tab:mps)).

TODO: write more here

```{r mps, results='asis'}
mp <- readr::read_csv(here("mse",  "mp.txt"), comment = "#")
csasdown::csas_table(mp, caption = "Candidate MPs.", col_names = c("Management procedure", "MP type"))
```

## STEP 5: SIMULATION OF THE APPLICATION OF THE MANAGEMENT PROCEDURES  {#sec:results5}

```{r}
pm_df_list <- readRDS(here("mse/om/ye-pm-all.rds"))
pm_all <- bind_rows(pm_df_list, .id = "scenario")
pm_avg <- group_by(pm_all, MP) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE)
pm_min <- group_by(pm_all, MP) %>%
  summarise_if(is.numeric, min, na.rm = TRUE)
reference_mp <- c("FMSYref75", "NFref", "FMSYref")

satisficed_criteria <- c("LRP 1.5GT" = 0.9, "ST C10" = 0.5)
mp_sat <- dplyr::filter(pm_min, `LRP 1.5GT` > satisficed_criteria[1], `ST C10` > satisficed_criteria[2]) %>%
  pull(MP)
mp_sat <- mp_sat[!mp_sat %in% reference_mp]
mp_sat_b4_cut <- mp_sat

pm_avg_sat <- pm_avg %>% filter(MP %in% mp_sat)
pm_min_sat <- pm_min %>% filter(MP %in% mp_sat)
rversion <- gsub(" \\([0-9-]+\\)", "", devtools::session_info()$platform$version)
```

We ran the closed-loop simulations across 250 stochastic replicates using DLMtool version 5.4.2, `r rversion`, and the simulation random seed set to `1`, with the OMs and MPs described above.
We assessed convergence of the closed-loop simulation by plotting the cumulative performance metrics as replicates were added (Figure \@ref(fig:converge)).
We deemed 250 replicates sufficient since the rank order of MPs was consistent for this number of replicates (Figure \@ref(fig:converge)).

To determine which MPs would be carried forward as satisficed MPs [@miller2010], we began by assessing average and minimum performance across all candidate MPs for the reference set of OMs (Figures \@ref(fig:tigure-avg) and \@ref(fig:tigure-min)) .
To obtain a manageable number of MPs for further consideration, we set satisficing thresholds of `r names(satisficed_criteria)[1]` $>$ `r satisficed_criteria[[1]]` and `r names(satisficed_criteria)[2]` $>$ `r satisficed_criteria[[2]]`.
We chose to apply the satisficing criteria to the average performance metrics.
Applying these criteria resulted in `r gfutilities::number_to_word(length(mp_sat))` remaining MPs (`r gfutilities::commify(sort(mp_sat))`).

(ref:fig-converge)
Assessing convergence of the closed-loop simulations on consistent rank order of MPs within performance metrics.
Colours represent individual satisficed and reference MPs.
Lines that do not cross by the final replicates indicate that rank order among replicates has converged.
Although not shown, we also checked that the satisficing rules had converged (i.e., the selection of satisficed MPs did not change with additional replicates).

```{r converge, fig.cap="(ref:fig-converge)", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-convergence.png"))
```

(ref:fig-tigure-avg) Average performance of all candidate MPs across the reference set of OMs.
MPs are ordered by decreasing performance metric values from top to bottom starting with the left-most performance metric (LT LRP) and using columns from left to right to break any ties.
The colour shading reflects the probabilities.
Outlined cells represent MPs that met a particular performance metric's satisficing criteria.
Using this set of criteria, MPs would be "satisficed" if cells in *both* "`r names(satisficed_criteria)[1]`" and "`r names(satisficed_criteria)[2]`" were outlined.
Light grey MPs indicate reference MPs.

```{r tigure-avg, fig.cap="(ref:fig-tigure-avg)", out.width="3.5in"}
knitr::include_graphics(here("mse/figures/ye-tigure-refset-avg.png"))
```

(ref:fig-tigure-min) Minimum performance of all candidate MPs across the reference set of OMs.
This figure is the same as Figure \@ref(fig:tigure-avg) but shows the **minimum** performance metric across the OMs in the reference set for the purposes of applying satisficing rules.
In other words, this figure illustrates the worst performance of each MP across the reference set of OMs.

```{r tigure-min, fig.cap="(ref:fig-tigure-min)", out.width="3.5in"}
knitr::include_graphics(here("mse/figures/ye-tigure-refset-min.png"))
```

\clearpage

## STEP 6: PRESENTATION OF RESULTS AND SELECTION OF MANAGEMENT PROCEDURE  {#sec:results6}

### Reference Set Results

Performance within the reference set of OMs varied across MPs and performance metrics (Figures \@ref(fig:tigure-avg), \@ref(fig:tigure-min), \@ref(fig:tigure-panel), and \@ref(fig:dot-lines)).
There was variability in performance among the individual reference-set scenarios with the most variability in both the long-term and entire projection period performance metrics: TODO (Figure \@ref(fig:tigure-panel)).
Between the two performance metrics used in the initial satisficing step, average LRP 1.5GT varied between `r round2(min(pm_avg_sat$"LRP 1.5GT"))` and `r round2(max(pm_avg_sat$"LRP 1.5GT"))`, and ST C10 varied between `r round2(min(pm_avg_sat$"ST C10"))` and `r round2(max(pm_avg_sat$"ST C10"))` across MPs within the reference set (Figure \@ref(fig:tigure-avg) and \@ref(fig:tigure-panel)).
Minimum LRP 1.5GT varied between `r round2(min(pm_min_sat$"LRP 1.5GT"))` and `r round2(max(pm_min_sat$"LRP 1.5GT"))`, and ST C10 varied between `r round2(min(pm_min_sat$"ST C10"))` and `r round2(max(pm_min_sat$"ST C10"))` across MPs within the reference set (Figure \@ref(fig:tigure-min) and \@ref(fig:tigure-panel)).

A dot-and-line plot of the performance metrics aggregated across scenarios helps compare performance across satisficed MPs (Figure \@ref(fig:dot-lines)).
TODO: explain more results here

Trade-offs among performance metrics for most of the satisficed MPs were TODO (add something here) within the reference-set scenarios (Figures \@ref(fig:tradeoff-reference) and \@ref(fig:spider-satisficed-mps-avg)).
TODO (write something more here).

The timeseries trajectories of the projected survey index (Figure \@ref(fig:proj-index), *B*/*B*~MSY~, *F*/*F*~MSY~, and catch further demonstrate performance across the various MPs and reference set OMs (Figures \@ref(fig:proj-updog-fixsel)--\@ref(fig:proj-no-cpue)).
TODO (write something more here if desired).

Kobe plots demonstrate either the final *B*/*B*~MSY~ vs. *F*/*F*~MSY~ status among replicates (Figure \@ref(fig:kobe)) or the trajectory of these stock status values through time (Figure \@ref(fig:worm)).
TODO (write something more here if desired).

(ref:fig-tigure-panel) Performance of satisficed MPs for the reference set OMs.
MPs are ordered by decreasing performance metric values from the averaged reference set (Figure \@ref(fig:tigure-avg).
These are the same data underlying Figure \@ref(fig:tigure-avg) and Figure \@ref(fig:tigure-min) but shown for individual reference-set scenarios and only for satisficed MPs.

```{r tigure-panel, fig.cap="(ref:fig-tigure-panel)", out.width="0.8\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-tigure-refset.png"))
```

\clearpage

(ref:fig-dot-lines)
Dot-and-line plot of performance metrics across scenarios.
Dots represent average performance metric values and thin lines represent the range of values across scenarios.
Thick lines represent the range of values across scenarios after dropping the high and low values.

```{r dot-lines, fig.cap="(ref:fig-dot-lines)", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-dot-refset-avg.png"))
```

\clearpage

```{r tradeoff-reference, fig.cap="Trade-off between LT LRP and STC average performance metrics across the reference-set scenarios.", out.width="5in"}
knitr::include_graphics(here("mse/figures/ye-tradeoff-refset-avg.png"))
```

\clearpage

```{r spider-satisficed-mps-avg, fig.cap="Radar-plot representation of average performance metric trade-offs for the reference-set scenarios. The outside of the hexagon represents a performance metric probability of 1.0 in the middle represents a value of 0. Dashed lines represent reference MPs.", out.width="5in"}
knitr::include_graphics(here("mse/figures/ye-radar-refset-avg.png"))
```

\clearpage

```{r proj-index, fig.cap="Historical and projected HBLL-inside survey relative abundance index values. Vertical dashed line represents 2019. The shaded region represents the 95\\% quantile and individual lines represent four sample replicates.", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-projections-index.png"))
```

\clearpage

(ref:fig-proj-updog-fixsel) B/B~MSY~, F/F~MSY~, and catch from the historical and projected time periods for the "Upweight dogfish" OM.
Dark line indicates the median value and the darker and lighter shaded ribbons indicate the 50% and 90% quantiles.
Thin gray lines represent illustrative simulation replicates.
The vertical dashed line indicates the last year of the historical period.
The horizontal dashed lines indicate B/B~MSY~ = 0.8 and 0.4 and F/F~MSY~ = 1.

```{r proj-updog-fixsel, fig.cap="(ref:fig-proj-updog-fixsel)", out.width="6.5in"}
knitr::include_graphics(here("mse/figures/ye-projections-updog_fixsel.png"))
```

(ref:fig-proj-low-catch) Same as Figure \@ref(fig:proj-updog-fixsel) but for the OM "Low catch".

```{r proj-low-catch, fig.cap="(ref:fig-proj-low-catch)", out.width="6.5in"}
knitr::include_graphics(here("mse/figures/ye-projections-lowcatch_fixsel.png"))
```

(ref:fig-proj-episodic-recruitment) Same as Figure \@ref(fig:proj-updog-fixsel) but for the OM "Episodic recruitment".

```{r proj-episodic-recruitment, fig.cap="(ref:fig-proj-episodic-recruitment)", out.width="6.5in"}
knitr::include_graphics(here("mse/figures/ye-projections-episodic_recruitment.png"))
```

(ref:fig-proj-upweight-dogfish) Same as Figure \@ref(fig:proj-updog-fixsel) but for the OM "Higher steepness".

```{r proj-upweight-dogfish, fig.cap="(ref:fig-proj-upweight-dogfish)", out.width="6.5in"}
knitr::include_graphics(here("mse/figures/ye-projections-upweight_dogfish.png"))
```


\clearpage

(ref:fig-kobe) B/B~MSY~ and F/F~MSY~ values from the final year of the projections across all replicates.
Dots represent individual replicates.
Contour lines indicate two-dimensional kernel-density-smoothed quantiles at 0.25, 0.50, and 0.75 levels, calculated in log space.

```{r kobe, fig.cap="(ref:fig-kobe)", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-kobe.png"))
```

\clearpage

(ref:fig-worm) Trajectory of B/B~MSY~ and F/F~MSY~ values summarized across replicates.
The solid line corresponds to the median value.
Each diamond represents the 50% quantile of B/B~MSY~ (horizontal) and F/F~MSY~ (vertical).

```{r worm, fig.cap="(ref:fig-worm)", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-worms.png"))
```

\clearpage

### Robustness Set Results

*TODO: Refer directly to YE results. This is all rex even though the plots say Yelloweye*

The "No CPUE Ceq. 50%" robustness scenario represents a markedly different biomass and fishing mortality historical trajectories to any of the reference set scenarios.
This scenario estimates the stock to be smaller (smaller $R_0$ values in Figure \@ref(fig:sra-conditioned-parameters)) with high $F_t$ values around 2005 (Figure \@ref(fig:F-om)).
In this scenario, the Itarget MPs are able to achieve LT LRP $>$ 0.60, while the constant catch and surplus production model achieve LT LRP $<$ 0.35 (Figures \@ref(fig:tigure-panel-rob) and \@ref(fig:dots-satisficed-mps-robust)).
Other performance metrics show less contrast among the MPs, although LT USR also remains higher for the Itarget vs. constant-catch or surplus-production MPs (Figure \@ref(fig:tigure-panel-rob) and \@ref(fig:dots-satisficed-mps-robust)).

This scenario demonstrates stronger trade-offs among performance metrics than the reference set scenarios (Figures \@ref(fig:tradeoff-robust) and \@ref(fig:spider-satisficed-mps-robust)).
In particular, there is an obvious trade-off between STC and LT LRP (Figures \@ref(fig:tradeoff-robust)).
The constant catch MPs achieved the highest STC values but at the expense of lower LT LRP values.
The Itarget MPs struck more of a balance between LT LRP and STC (Figures \@ref(fig:tradeoff-robust)).

For the "M increasing" scenario, the satisficed MPs were fairly robust to linear increases in natural mortality in the projection period, with the exception that all satisficed MPs had a considerably lower probability of achieving the long-term catch (LTC) metric (Figures \@ref(fig:tigure-panel-rob)--\@ref(fig:spider-satisficed-mps-robust)).
Notably, the Itarget satisficed MPs maintained LT LRP $>$ 0.9 despite increasing $M$, whereas the fixed-catch and surplus-production MPs all resulted in LT LRP $<$ 0.9.
The Itarget MPs were able to ramp down catch as $M$ increased (Figure \@ref(fig:proj-high-index-cv)) to maintain most replicates above the LRP.

By plotting the projections from all scenarios (Figures \@ref(fig:proj-updog-fixsel)--\@ref(fig:upweight-dogfish)) on the same panels, we can examine the sensitivity of the projections to the various OM scenario conditions (Figure \@ref(fig:proj-scenarios)).
The historical trajectories were most sensitive to the assumptions of the "No CPUE Ceq. 50%" scenario compared to the other scenarios.
The projected trajectories were relatively similar across all reference-set scenarios, while differing for the "No CPUE Ceq. 50%" and "M increasing" robustness scenarios.
Note that sensitivity of the projections does not necessarily correspond to sensitivity of the rank order of MPs (Figure \@ref(fig:tigure-panel-rob)).

(ref:fig-tigure-panel-rob) Performance of satisficed MPs for the robustness set OMs.

```{r tigure-panel-rob, fig.cap="(ref:fig-tigure-panel-rob)", out.width="5.5in"}
knitr::include_graphics(here("mse/figures/ye-tigure-robset.png"))
```

```{r dots-satisficed-mps-robust, fig.cap="Performance of satisficed MPs for the robustness set OMs.", out.width="0.9\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-dot-robset.png"))
```

```{r tradeoff-robust, fig.cap="Trade-off plot between LT LRP and STC performance metric values for satisficed MPs for the robustness set OMs.", out.width="6in"}
knitr::include_graphics(here("mse/figures/ye-tradeoff-robset.png"))
```

```{r spider-satisficed-mps-robust, fig.cap="Radar-plot representation of the trade-off in performance metric values for satisficed MPs for the robustness set OMs.", out.width="\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-radar-robset.png"))
```

(ref:fig-proj-low-m) Same as Figure \@ref(fig:proj-updog-fixsel) but for the OM "Low M".

```{r proj-low-m, fig.cap="(ref:fig-proj-low-m)", out.width="6.5in"}
knitr::include_graphics(here("mse/figures/ye-projections-lowM_fixsel.png"))
```

(ref:fig-proj-high-index-cv) Same as Figure \@ref(fig:proj-updog-fixsel) but for the OM "High CV HBLL projected".

```{r proj-high-index-cv, fig.cap="(ref:fig-proj-high-index-cv)", out.width="6.5in"}
knitr::include_graphics(here("mse/figures/ye-projections-high_index_cv.png"))
```

(ref:fig-proj-scenarios) B/B~MSY~, F/F~MSY~, and catch from the historical and projected time periods.
Colours represent the reference- and robustness-set scenarios.
Lines represent medians and shaded regions represent 50% quantiles.
Satisficed and reference MPs are shown from top to bottom.

```{r proj-scenarios, fig.cap="(ref:fig-proj-scenarios)", out.width="0.9\\textwidth"}
knitr::include_graphics(here("mse/figures/ye-projections-scenarios.png"))
```


